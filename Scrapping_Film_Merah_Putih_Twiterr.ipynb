{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHAV6moIFKjiVujzMctc+T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hafizh-Al-Muzakar/Scrapping-film-merah-putih-one-for-all-twitter/blob/main/Scrapping_Film_Merah_Putih_Twiterr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLjveWrKrt5r",
        "outputId": "18adb09e-752b-46bb-82a7-9c93ca620149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting twscrape\n",
            "  Downloading twscrape-0.17.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting aiosqlite>=0.17.0 (from twscrape)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from twscrape) (4.13.4)\n",
            "Collecting fake-useragent>=1.4.0 (from twscrape)\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: httpx>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from twscrape) (0.28.1)\n",
            "Collecting loguru>=0.7.0 (from twscrape)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pyotp>=2.9.0 (from twscrape)\n",
            "  Downloading pyotp-2.9.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite>=0.17.0->twscrape) (4.14.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.13.0->twscrape) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.26.0->twscrape) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.26.0->twscrape) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.26.0->twscrape) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.26.0->twscrape) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.26.0->twscrape) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.26.0->twscrape) (1.3.1)\n",
            "Downloading twscrape-0.17.0-py3-none-any.whl (39 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyotp-2.9.0-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: pyotp, loguru, fake-useragent, aiosqlite, twscrape\n",
            "Successfully installed aiosqlite-0.21.0 fake-useragent-2.2.0 loguru-0.7.3 pyotp-2.9.0 twscrape-0.17.0\n"
          ]
        }
      ],
      "source": [
        "pip install twscrape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from twscrape import API\n",
        "from twscrape.logger import set_log_level\n",
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "id": "Ph2fvf37sPKO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accounts = []\n",
        "LIMIT_TWEET = 10000"
      ],
      "metadata": {
        "id": "OzzLRuLRsVir"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"scrapping/accounts.json\" , \"r\") as f:\n",
        "  accounts = json.load(f)\n",
        "if not accounts:\n",
        "  print(\"No account found in account.json. please add account to scrape\")\n",
        "  exit(1)"
      ],
      "metadata": {
        "id": "p2aOpm3quXHB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def scrape_tweets(api, start_date, end_date, save_to_file):\n",
        "  tweets = []\n",
        "\n",
        "  q = f\"merah putih one for all since:{start_date} until:{end_date}\"\n",
        "  print(f\"Scapping tweets : {q}\")\n",
        "\n",
        "  async for tweet in api.search(q, limit = LIMIT_TWEET):\n",
        "    c =[\n",
        "        tweet.id,\n",
        "        tweet.date,\n",
        "        tweet.url,\n",
        "        tweet.user.username,\n",
        "        tweet.rawContent,\n",
        "        tweet.likeCount,\n",
        "        tweet.replyCount,\n",
        "        tweet.quoteCount,\n",
        "        tweet.retweetCount\n",
        "    ]\n",
        "    tweets.append(c)\n",
        "\n",
        "  df = pd.DataFrame(tweets,\n",
        "                    columns = [\"tweet_id\",\"time_created\",\"url\",\"username\",\"tweet\",\"like_count\",\"reply_count\",\"quote_count\",\"retweet_count\"],\n",
        "                    )\n",
        "  file_path = \"scrapping/results/\" + save_to_file\n",
        "\n",
        "  df.to_csv(file_path, index = False)\n",
        "  print(f\"Saved {len(df)} tweets to {file_path}\")\n",
        "\n",
        "  return len(df)"
      ],
      "metadata": {
        "id": "p4aUCdZIxZOH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    api = API()\n",
        "\n",
        "    for account in accounts:\n",
        "        cookies = f\"auth_token={account['auth_token']}; ct0={account['ct0']}\"\n",
        "        await api.pool.add_account(\n",
        "            account[\"username\"],\n",
        "            account[\"password\"],\n",
        "            account[\"email\"],\n",
        "            account[\"email_password\"],\n",
        "            cookies=cookies,\n",
        "        )\n",
        "\n",
        "    await api.pool.login_all()\n",
        "\n",
        "    set_log_level(\"INFO\")\n",
        "    print(\"Starting scraping...\")\n",
        "\n",
        "    date_ranges = [\n",
        "        (\"2025-07-01\", \"2025-08-15\", \"tweets-merah-putih-one-for-all-2024-07.csv\"),\n",
        "\n",
        "    ]\n",
        "\n",
        "    total_count = 0\n",
        "    for start_date, end_date, file_name in date_ranges:\n",
        "        count = await scrape_tweets(api, start_date, end_date, file_name)\n",
        "        total_count += count\n",
        "\n",
        "\n",
        "    print(f\"\\nüéâ Scraping completed. Total tweets scraped: {total_count}\")"
      ],
      "metadata": {
        "id": "ceWhAkCi356D"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oUSfsfF_TOx",
        "outputId": "6309578b-602e-4570-9a51-081d693b22a4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-08-15 12:28:16.113\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36madd_account\u001b[0m:\u001b[36m88\u001b[0m - \u001b[33m\u001b[1mAccount HMuzakar86311 already exists\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scraping...\n",
            "Scapping tweets : merah putih one for all since:2025-07-01 until:2025-08-15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-08-15 12:28:48.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mNo account available for queue \"SearchTimeline\". Next available at 12:43:16\u001b[0m\n",
            "\u001b[32m2025-08-15 12:43:20.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mContinuing with account HMuzakar86311 on queue SearchTimeline\u001b[0m\n",
            "\u001b[32m2025-08-15 12:43:50.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mNo account available for queue \"SearchTimeline\". Next available at 12:58:20\u001b[0m\n",
            "\u001b[32m2025-08-15 12:58:22.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mContinuing with account HMuzakar86311 on queue SearchTimeline\u001b[0m\n",
            "\u001b[32m2025-08-15 12:58:53.247\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mNo account available for queue \"SearchTimeline\". Next available at 13:13:22\u001b[0m\n",
            "\u001b[32m2025-08-15 13:13:24.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mContinuing with account HMuzakar86311 on queue SearchTimeline\u001b[0m\n",
            "\u001b[32m2025-08-15 13:13:54.999\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mNo account available for queue \"SearchTimeline\". Next available at 13:28:24\u001b[0m\n",
            "\u001b[32m2025-08-15 13:28:26.309\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mContinuing with account HMuzakar86311 on queue SearchTimeline\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 4632 tweets to scrapping/results/tweets-merah-putih-one-for-all-2024-06.csv\n",
            "\n",
            "üéâ Scraping completed. Total tweets scraped: 4632\n"
          ]
        }
      ]
    }
  ]
}